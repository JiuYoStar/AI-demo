

# Prompt

## 一、模型使用方式

**Q：我们是在终端用 Ollama，还是用 Ollama 客户端？**

* 两种方式都可以。

  * ✅ **终端命令行方式**：直接使用 `ollama` 命令与本地模型交互（常用于开发者）。
  * ✅ **Ollama 客户端**：图形界面更友好，适合测试与快速验证。
* 同时还可以通过 **Python API** 调用 Ollama 已部署的大模型，实现编程接口调用。

---

## 二、显卡与模型容量

**Q：4090 显卡可以部署多大的模型？**

* 一般可以运行 **7B 或 8B 参数规模**的模型。
* 如果模型采用 **激活参数（如 A3B）结构**，实际显存占用会更低。

---

## 三、DeepSeek 模型与 R1 机制

### 1. 模型说明

* **DeepSeek-R1**：开源、免费商用的推理强化模型。
* **模型结构示例**：

  * `DeepSeek-R1-Distill-Qwen-7B`
  * `Qwen3-30B-A3B-Instruct-2507`

    > 30B 为全量参数规模，A3B 表示仅有 3B 激活参数。

### 2. 模型输出格式

```xml
<think> 思考过程 </think>
<answer> 最终答案 </answer>
```

> `<think>` 用于展示推理草稿，`<answer>` 为最终输出。

### 3. 训练方法

* **方法1：人工标注**
* **方法2：强化学习（R1-Zero）**
  模型自我探索，AI 自动生成 `<think>` 和 `<answer>`，筛选出高质量样本用于监督训练。

---

## 四、R1 模型训练逻辑（理科生类比）

| 模型类型    | 特征         | 思维方式               |
| ----------- | ------------ | ---------------------- |
| **V3 模型** | 直接回答问题 | 类似文科生             |
| **R1 模型** | 先思考再回答 | 类似理科生，有“草稿纸” |

> `<think>` 部分相当于理科生写草稿的过程。

---

## 五、监督学习 vs 强化学习

| 类型                         | 特征                                | 优缺点                       |
| ---------------------------- | ----------------------------------- | ---------------------------- |
| **监督学习（SFT）**          | 由人工提供标准答案                  | 收敛快，但容易受限于人类知识 |
| **强化学习（RLHF / RLAIF）** | AI 自己探索更优解，通过奖励函数学习 | 慢但上限更高                 |

**理解要点：**

* 监督学习：答案 = 学习方向
* 强化学习：模型自行尝试并通过“奖励函数”判断好坏。

---

## 六、奖励函数机制（以 R1 为例）

奖励函数常依据以下标准评分：

1. 是否包含 `<think>` 与 `<answer>`；
2. 答案是否正确；
3. 是否符合 XML 规范。

> 早期奖励由人工评估（例如 OpenAI 初期），现在可用大模型辅助评估。

---

## 七、模型下载与调用

### 1. 在 ModelScope 下载

```python
from modelscope import snapshot_download
snapshot_download('deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', cache_dir="/root/autodl-tmp/models")
```

### 2. 在 Ollama 拉取模型

```bash
ollama pull deepseek-r1:1.5b
```

### 3. 模型存储位置

* Windows: `C:\Users\<用户名>\.ollama\models`
* Linux: `~/.ollama/models`

---

## 八、模型名称来源

* Ollama 模型搜索: [https://ollama.com/search](https://ollama.com/search)
* ModelScope 模型搜索: [https://modelscope.cn/search?search=deepseek-r1](https://modelscope.cn/search?search=deepseek-r1)

---

## 九、Python 任务示例

**示例 1：结构化财报提取**

> 从财报中提取公司名称、股票代码、营收、净利润、毛利、总资产、总负债，输出 JSON。

```json
{"公司名称": "伊利集团", "股票代码": "600887", "营收": "1234亿", "净利润": "89亿", "毛利": "450亿", "总资产": "3000亿", "总负债": "1200亿"}
```

**示例 2：数学计算助手**

> 加 1000，减 500，乘 1.2
> 输入：1500, 2500, 3500 → 输出：2400, 3600, 4800

**示例 3：客户投诉助手**

> 自动生成工单编号、分配部门。

---

## 十、Function Calling 与代码执行

* AI 可以通过 **Function Calling** 调用接口（例如创建工单、获取数据）。
* **Cursor 编辑器** 可集成本地模型辅助编程，例如 wucai code 项目。
* 可在 **Node.js / Python** 中封装自定义 AI 产品接口。

---

## 十一、平台区别

| 平台                     | 特点                                |
| ------------------------ | ----------------------------------- |
| **阿里云百炼 (Bailian)** | 模型在线 API 调用平台，适合生产集成 |
| **ModelScope**           | 模型下载与研究社区                  |
| **Ollama / vLLM**        | 本地或企业内部模型部署工具          |

---

## 十二、提示词工程与上下文工程

| 概念                                 | 说明                                                         |
| ------------------------------------ | ------------------------------------------------------------ |
| **提示词工程 (Prompt Engineering)**  | 通过优化提示词，引导模型生成更符合预期的回答。               |
| **上下文工程 (Context Engineering)** | 从系统视角设计信息输入与输出的整体流程（例如 Coze Agent、工作流编排）。 |

> 字节的 **Prompt Pilot**、Coze 智能体 都是基于提示词工程的产品。

---

## 十三、提示词迁移与测试

* 优化过的提示词在 **小模型上验证 → 大模型上使用**，一般是正向收益。
* 上线前应通过测试集验证结果稳定性。

---

## 十四、微调模型迁移

* 微调后的参数已融合进模型权重。
* 迁移时只需移动模型权重文件，无需单独迁移训练数据。

---

## 十五、本地部署与资源管理

* 部署好的模型自带预训练知识，无需额外训练。
* 若 Ollama 退出仍占内存，可能是后台进程未关闭。
* Ollama 默认端口：`11434`，可通过环境变量修改：

  ```bash
  export OLLAMA_HOST=0.0.0.0:12434
  ```

---

## 十六、模型部署方案比较

| 部署方式                 | 特点                       | 适用场景        |
| ------------------------ | -------------------------- | --------------- |
| **代码执行（API 模式）** | 可扩展，可封装产品         | SaaS / 企业应用 |
| **Ollama**               | 本地轻量部署，适合开发测试 | 个人 / 小型企业 |
| **vLLM**                 | 高性能并发部署             | 企业内部服务端  |

---

## 十七、知识延伸

| 主题                 | 简要说明                                                     |
| -------------------- | ------------------------------------------------------------ |
| **强化学习推荐书籍** | 《Reinforcement Learning: An Introduction》（Sutton & Barto） |
| **蒸馏 vs 奖励模型** | 蒸馏是知识迁移；奖励模型是“好坏评判”标准                     |
| **并发与线程**       | LLM 本身是神经网络，可通过 FastAPI 等方式支持多线程并发请求  |
| **并发限制**         | 由请求数和上下文窗口大小共同决定                             |

---

# ✅ 总结

| 模块     | 工具                                      | 功能定位         |
| -------- | ----------------------------------------- | ---------------- |
| 模型管理 | **Ollama / vLLM**                         | 本地或服务端部署 |
| 模型调用 | **Python / API / Node.js**                | 执行与集成       |
| 模型来源 | **ModelScope / 百炼**                     | 下载与在线使用   |
| 能力扩展 | **Function Calling / Prompt Engineering** | 企业级应用构建   |
| 模型优化 | **蒸馏 + 强化学习 + 奖励函数**            | 提升推理能力     |
| 应用落地 | **Coze / wucai code / Cursor**            | 智能体与代码助手 |
