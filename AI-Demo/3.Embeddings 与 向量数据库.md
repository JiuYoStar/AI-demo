# 📘 Embeddings 与 向量数据库

## 1. 为什么需要 Embedding？

LLM（例如 Qwen / DeepSeek / GPT）本质上是一个 **问答模型**：输入 Query → 输出答案。

但企业内部知识库有这些特点：

* 文档巨大：PDF/Word/网页几百 MB
* 结构复杂：表格、段落、代码片段
* 问题无法直接复制文章内容
* 如果直接把全部文档丢给 LLM，上下文窗口不够、成本极高。

所以要解决“**我问一个问题 → 找到知识库里最相关的几段内容**”，必须要：

### 1）把文本 → 变成数字向量（embedding 向量）

### 2）存入向量数据库

### 3）问题向量化 → 找最相似的知识内容 → 再喂给 LLM

这就是 **现代企业级智能客服 / 智能问答 / RAG 系统** 的核心流程。

---

## 2. 什么是 Embedding？

Embedding 是把 **文本** 映射成一个 **连续的高维向量**，比如：

| 输入文本 | Embedding 维度     |
| ---- | ---------------- |
| 一个单词 | 384 / 768 / 1024 |
| 一句话  | 384 / 768 / 1024 |
| 一段长文 | 384 / 768 / 1024 |

例如：

```
"苹果" → [0.12, -0.88, ...., 1.34]  # 长度假设为 768
```

💡 **Embedding 的含义：表示这个文本的“语义特征数字卡”。**

相似文本 → Embedding 向量靠得近
不相似 → 距离远

这就是向量检索的基础。

---

## 3. embedding 模型 和 大模型（LLM）的区别

| 类型                                                    | 功能          | 用途               |
| ----------------------------------------------------- | ----------- | ---------------- |
| **LLM（Qwen、DeepSeek）**                                | 生成回答、写代码、推理 | Query → Response |
| **Embedding 模型（sentence-transformer、qwen-embedding）** | 把文本转成向量     | 相似度检索、RAG        |
| **Word2Vec/Glove**                                    | 老式词向量，词级别   | NLP 初级教学用途       |

所以：

### ❌ LLM ≠ Embedding

### ✔ LLM 可以使用 embedding 作为上下文（RAG）

### ✔ embedding 模型通常更小、更便宜、速度更快

---

## 4. Embeddings 与 Word2Vec 的关系

### Word2Vec

* 词级别
* 无监督（根据邻居 co-occurrence 学）
* 输出一个词 → 一个向量
* 维度通常 100 ~ 300

### 新式 embedding（sentence-transformer 或 qwen-embedding）

* 句子级、段落级
* 大量语料预训练
* 用于检索、排序、RAG
* 维度 384 - 1024
* 精度远高于 word2vec

### Word2Vec 为什么要看“邻居”？

Word2Vec 的训练目标：
**如果 A 和 B 经常一起出现，那它们的向量要接近。**

例：
“我/喜欢/吃/苹果/和/香蕉”
“我/喜欢/吃/苹果/和/橘子”

苹果的邻居（window=2）统计为：

```
喜欢 2
吃 2
和 2
香蕉 1
橘子 1
```

**这就是特征工程本质：用词 co-occurrence 去构造特征。**

embedding 模型做的是更高级、更抽象的版本。

---

## 5. 为什么要 N-gram？（传统特征工程的回顾）

传统 NLP：(NLP 👉🏻 natural language progressing 自然语言处理)

* 1-gram： 单词本身
* 2-gram： 连续两个词
* 3-gram： 连续三个词

所有可能出现的 N-gram → 成为特征维度。

例如酒店描述：

* 所有酒店合计可能有 3347 个 Ngram 特征
* 每个酒店被表示成一个 3347 维向量
* 大部分是 0 → 因为文本很稀疏
* 这就是 “稀疏向量”

缺点：维度太高（几千～几十万）

Embedding 的优势：
✔ 从稀疏 → 压缩成密集向量（例如 768 维）
✔ 保留语义信息
✔ 计算快、存储体积小
✔ 支持相似度检索

---

## 6. 向量数据库（FAISS / Milvus / PGVector）

向量数据库不是普通数据库：

### 它提供的是：

* 快速向量相似度检索
* 近似最近邻（ANN）搜索
* 一秒查百万向量
* 支持向量插入、删除、更新

### 典型存储内容：

| 字段       | 内容                  |
| -------- | ------------------- |
| id       | 唯一编号                |
| vector   | embedding 向量        |
| text     | 原始文本                |
| metadata | 来源文件、页码、作者、大类、业务标签… |

示例（迪士尼）：

```
ID=2
text="对于在线购买的迪士尼门票..."
metadata = {
  source: "online_policy.html",
  category: "退票政策",
  author: "E-commerceTeam"
}
```

这些 metadata 非常重要：
✔ 用于过滤
✔ 用于展示结果
✔ 用于增加可信度
✔ 用于构建企业知识图谱

---

## 7. 把 RAG 系统打通（完整流程图）

可用于画 SVG：

```
           ┌──────────────┐
           │ 原始文档(PDF) │
           │  Word/网页    │
           └───────┬──────┘
                   │ 转文本
                   ▼
           ┌──────────────┐
           │   文本切 chunk │
           └───────┬──────┘
                   │
                   ▼
        ┌─────────────────────┐
        │ embedding模型        │
        │ (qwen-embed / SBERT) │
        └─────────┬───────────┘
                  │  embedding
                  ▼
        ┌─────────────────────┐
        │  向量数据库 FAISS/M │
        │  存 vector + text   │
        └─────────┬───────────┘
                  │
        用户 Query │
                  ▼
      ┌──────────────────────┐
      │ Query → embedding     │
      └───────────┬──────────┘
                  ▼ 相似度检索
        ┌─────────────────────┐
        │  top-k 文本片段      │
        └─────────┬───────────┘
                  │
                  ▼
      ┌──────────────────────┐
      │ LLM (DeepSeek/Qwen)   │
      │   Query + 文本作为上下文│
      └───────────┬──────────┘
                  ▼
           最终回答输出
```

---

## 8. 企业为什么要保存向量数据库？

* 专属于企业的知识
* 企业文档每天更新
* 自己的系统要保证 **可控、可审计、可隐私**
* 向量数据库可本地化部署
* 允许不断增加新的文档（增量 embedding，而不是重训练）

✔ 本地存储
✔ 内网使用
✔ 快速检索
✔ 不依赖外部 API

---

## 9. 常见工程问题答疑

### ❓ 一个词多个意思怎么办？

现代 embedding 模型是 **句子级别**，会根据上下文自动 disambiguate。

“like this” → 像
“I like you” → 喜欢

embedding 不会混淆。

### ❓ 企业应用里 LLM 会用向量数据库吗？

只有在 RAG 系统中会用。
LLM 本身不包含向量数据库，也不是固定的。

### ❓ embedding 后新增文本需要重新训练吗？

不需要。
新的文本 → 直接 embedding → 插入向量数据库即可。

### ❓ 可以反向找原始内容吗？

可以，因为 metadata 和文本都存着。

### ❓ embedding 是自动生成的还是人工规定的？

模型自动学习得到，无需人工规定。

### ❓ Excel 要怎么 embedding？

必须先：

* 结构化提取
* 或转换为文本描述（例如：列名 + 行值）

---

## 10. 最终总结（非常重要）

你可以用一句话总结整套系统：

> Embedding 是文本的语义向量，“给文本打一张特征卡”；
> 向量数据库是存卡的柜子，用来快速找最相似的卡；
> LLM 则是最终做推理、生成答案的“解释器”。
> 三者组合就是现代 RAG 系统。

---

## 11. 为什么embedding相比LLM比较小, 比较便宜?
  1. 任务目标

    LLM: 生成文本, 难度较大
      - 长距离语言理解
      - 推理
      - 思维链
      - 代码生成
      - 创造性内容
      - 常识 / 世界知识
      - 多轮对话保持记忆
      - 格式化输出
      - 模拟人类的思考与写作方式
      👉 **LLM 必须包含大量知识、复杂逻辑结构，需要巨大的参数量去“记住”和“推理”**
    Embedding: 生成向量, 难度较小
      - 把语义压缩成向量
      - 语义相似的文本 → 向量靠近
      - 不相似的 → 向量远离
      - 不需要生成句子
      - 不需要深度推理
      - 不需要创造力
      - 不需要对世界知识有完备记忆
      👉 **捕捉语义,保持向量的空间结构**
  2. 结构差异

    ```python Embedding
      文本 →  tokenizer → encoder → 向量输出
    ```
    ```python LLM
      文本 → tokenizer → decoder（多层 transformer block）
        → 每一步都预测下一个 token
        → 长上下文注意力
        → 多头注意力深度计算
        → 可能还包含 MoE（专家路由）
        → 输出文本
    ```
    **复杂度、运算量、需要的参数都差异巨大**
  3. 参数的数量和类型

    LLM: 上千亿的token(B), 参数类型多样, 包括权重矩阵、偏置、LayerNorm、激活函数等
    Embedding: 1～10 亿级, 参数类型单一, 主要是权重矩阵
    👉 **LLM 记住所有知识**
    👉 **Embedding 学会理解语义(分词)**
  4. 推理方式/成本

    LLM: 每输出一个token都要重新计算, 成本巨大 -> 每秒只能处理几十个token -> 一个token一个循环
    Embedding: 一次计算, 成本较小 -> 每秒可以处理几千个token -> 单向推理,不需要循环
  5. 模型对比

  | 模型                   | 参数量    | 用途      | 推理成本 |
  | -------------------- | ------ | ------- | ---- |
  | **Qwen-embedding-3** | ~300M  | 向量表示    | 低    |
  | **Bert-base**        | 110M   | encoder | 低    |
  | **MiniLM**           | 22M    | encoder | 极低   |
  | **Qwen2-7B**         | 7,000M | 生成式LLM  | 高    |
  | **DeepSeek-R1**      | 100B+  | 复杂推理生成  | 极高   |

  embedding 模型普遍只有 LLM 的：
    ✔ 1% ~ 30% 的参数量
    ✔ 0.5% ~ 1% 的成本

  *总结*
  LLM模型的工作: 理解语言 + 推理 + 记忆 + 创造 + 生成
  Embedding模型的工作: 将文本压缩成语义向量
  **难度和计算量级不在一个水平上**

