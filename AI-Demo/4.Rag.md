# 📘 **RAG（Retrieval-Augmented Generation）系统全解析**

>   ** RAG 技术入门与实践指南 **
------

# 📌 目录

1.  什么是 RAG？
2.  Embedding 与向量搜索（核心概念）
3.  Chunk 切片策略（怎么切、为什么切）
4.  向量数据库（作用与工作方式）
5.  检索过程：TopK / 阈值 / 重排序（Rerank）
6.  LLM 如何根据知识生成答案
7.  多模态 RAG（图片+文本）
8.  如何减少幻觉（面试高频）
9.  更新机制：增量更新 vs 全量更新
10.  实践 Q&A（你的所有问答）
11.  扩展：如何构建一个企业可用的 RAG 系统
------

# 📖 1. RAG 是什么？

**RAG = Retrieval（检索） + Augmented Generation（增强生成）**

一句话：

>  **RAG 是让 LLM 先“翻书找答案”，再基于找到的知识生成答案。**

流程如下：

```
准备阶段: 原始文档 → 切片 Chunk → Embedding → 向量库
   ↓
使用阶段: 用户问题 → Embedding → 相似度检索 → TopK chunks → LLM 生成答案
```
------

# 📐 2. Embedding（向量）是什么？

Embedding 是数字向量，例如：

```
[0.12, -0.02, 0.55, ..., 0.08]  # 1024维 / 不同的训练模型,维度不同
```

-   同义文本 👉🏻 向量接近 (绝对值)
-   不相关文本 👉🏻 向量远离 (绝对值)

用于：

-   相似度搜索
-   信息检索
-   文本聚类
-   多模态统一表达（CLIP 图像 → 向量）
------

# 📦 3. Chunk 切片（为什么要切？怎么切？）

因为 LLM 无法处理无限长文本，所以要将文档拆成小块。

### 切片策略

| Chunk 大小         | 适合场景           | 特点 |
| ------------------ | ------------------ | ---- |
| **300–500 tokens** | 找细节、具体内容   | 精准 |
| **600–800 tokens** | 综合问题           | 平衡 |
| **1000 tokens**    | 找整体概述、政策类 | 宽泛 |

### Overlap

避免一个知识点被切在两块之间：

```
Chunk1: ABCDEFG
Chunk2:    DEFGHI
Chunk3:       GHIKLM

# 在ragflow优化中, 重叠率在一定程度上提升召回块的准确率, 一般配置15%~20%, 可参考ragas对比记录
```
------

# 🗄 4. 向量数据库做什么？

**向量数据库 = embedding + 原文（metadata）**

功能：

-   存储向量
-   快速查相似度（百万级规模）
-   返回对应的原文 Chunk

本质上是一个“模糊搜索”系统。
------

# 🔍 5. 检索方式（TopK / 阈值 / Rerank）

### ① TopK：（最常用）

```
从1000万chunks → 取Top100 → 再取Top10
```

### ② 阈值过滤

如果相似度 < 0.3 就不要。

>   Q: 这里有一个问题, 如果召回得块相似度都比较低, 怎么处理?
>
>   A: 对路召回/改写query/反馈给用户避免幻觉/feedback👉🏻大模型直答案

### ③ Rerank

粗筛（向量） → 精排（更贵的模型）

例如：

```
粗筛：1000万 → 100
精排：100 → 10
```
------

# ✍️ 6. LLM 如何生成最终答案？

-   将 TopK chunk 拼接成背景知识
-   构造 Prompt
-   再让 LLM 生成答案

示例 Prompt：

```
请根据以下知识回答用户问题，只能使用知识中的内容：

[知识1]
...
[知识2]
...
用户问题：xxxx
```

这样可以显著减少幻觉。
------

# 🖼 7. 多模态 RAG

适合含有：

-   PDF 图片
-   扫描件
-   表格
-   海报
-   图文混排

工作方式：

1.  OCR → 得到文本
2.  CLIP → 得到图片向量（512维）
3.  文本向量 + 图片向量一起参与检索

Qwen-VL3 甚至可以一步到位：

-   读图片
-   读文字
-   QA
------

# 🛡 8. 如何减少幻觉？(*!!!*)

### 关键措施

1.  Prompt 里明确要求“只能根据知识回答”
2.  给模型加引用来源 `[1][2][3]`
3.  检索不到 → 明确回答“知识库中没有”
4.  控制上下文长度（避免污染）
5.  采用 Rerank（提高准确性）
6.  减少 chunk 内噪声（内容清洗）

👉🏻 **内容清洗 + prompt优化 + 限制来源 + 召回块排序**
------

# 🔁 9. 更新机制（企业级必备）

### 是否每次都要重新向量化？

不需要。

**采用增量更新：**

```
旧库：100个文件 → 已向量化
新增：1个文件 → 单独切片 → embedding → append → 索引refine
```
------

# 🎓 10. 常见问题补充

## Q：TF-IDF 特征也是词频吗？

是的，本质：

```
TF-IDF = TF（词频） × IDF（逆文档频率）
```

它是一种改良的词频表示。
------

## Q：训练是否一定不带随机性？

-   只要 **random_state 固定**
-   **模型结构相同**
-   **样本数据一致**

👉 **训练结果完全可复现**

深度学习也如此，只需要设定种子即可：

```
numpy, torch, random, cuda 都设随机种子
```
------

## Q：源文件切片后向量库取出的是否和源文件一样？

完全一样。

向量库存：

-   embedding 向量（用来检索）
-   原文文本（用来返回）
------

## Q：一个 chunk 的 embedding 维度是多少？

固定 1024（例如 text-embedding-v3）。
 无论原文：

-   1 个字
-   100 字
-   10000 字

👉 维度恒定不变
------

## Q：embedding 召回的 chunk 会不会含有错误？

可能会。但通过 rerank 提升准确性。
------

## Q：RAG 一定要清洗数据吗？

建议做：

-   去噪点
-   合理分段
-   Markdown 标题保留

但像密塔等系统会自动预处理，所以效果不错。
------

## Q：PDF 有图片怎么办？

-   用 OCR：提取文本
-   用 CLIP：提取视觉向量
-   或用 Qwen-VL 一步到位
------

## Q：如何决定 chunk 大小？

看任务：

-   文档类（规章制度） → 800–1000
-   笔记类、FAQ → 300–600
------

## Q：长篇小说 / 破案这种可以用 RAG 吗？

可以，但难点：

-   chunk 太大
-   内部关系复杂

推荐：

-   LLM 分段切割
-   章节级 chunk
-   使用全文 Embedding（bge-m3）
------

## Q：训练 Word2Vec 的参数如何选？

经验值：

```
vector_size = 100~300
window = 3~5
min_count = 2 或 5
```

但现在推荐直接使用现代 embedding（如 bge-large）。
------

## Q：为什么不能用模型训练来做分块？

可以，但：

-   成本高
-   模型不稳定
-   切分不一定符合你的业务

所以 90% 场景推荐规则分块 + overlap。
------

## Q：给 ChatGPT 一个论文问问题算 RAG 吗？

算。
RAG = “外部知识增强”。
------

# 📚 11. 企业级 RAG 架构

```
            ┌──────────┐
            │  原始文档 │
            └─────┬────┘
                  │  1. 文档解析（text + OCR + 表格）
                  ▼
            ┌──────────┐
            │ Chunk切分 │
            └─────┬────┘
                  │  2. Embedding
                  ▼
            ┌──────────┐
            │ 向量数据库 │  (Milvus / FAISS / PGVector)
            └─────┬────┘
                  │  3. 检索 TopK
                  ▼
            ┌──────────┐
            │Rerank重排 │
            └─────┬────┘
                  │  4. 构建 Prompt
                  ▼
            ┌──────────┐
            │   LLM生成 │
            └──────────┘
```
------

# 什么是特征工程?

特征工程是机器学习中的一个重要概念, 它指的是将原始数据转换为特征向量的过程。
特征工程的目的是将原始数据转换为可以被机器学习算法理解的形式, 从而提高模型的性能。
特征工程包括以下几个方面:
1. 特征选择: 选择最相关的特征, 减少特征的维度, 提高模型的性能。
2. 特征提取: 将原始数据转换为特征向量, 提高模型的性能。
3. 特征变换: 将特征向量转换为可以被机器学习算法理解的形式, 提高模型的性能。

# 参数最优的是什么?

没有最优的参数, 参数设置是一个玄学, 能用即可 + 多轮训练

# rag技术
在公司级使用中,微调的场景比较少,较多的是rag的优化
前期处理的占比比较大(解析/分词/分块/es/检索)
回答的占比比较小,都是在最后一部
---
rag解决的问题:
  1. 解决时效性的问题
  2. 减少模型幻觉  *公司级的大模型,禁止有幻觉回答*
  3. 提升专业领域的问答质量
---
rag的需求是否会导致上下文的缩减?
- 并不会, 上下文会根据实际情况进行调整
---
如果减少大模型的幻觉?
- 问答时添加上下问,添加限制
- 在答案中添加召回的引用
---
Rag的步骤:
  1. indexing 👉🏻 如何更好的把知识存起来
  2. retrieval 👉🏻 在海量数据中, 找到最有价值的内容(chunks) 👉🏻 相关性 + 重要性
  3. generation 👉🏻 结合用户的提问,与retrieval的内容,生成最终的答案
  以上3个步骤,是rag的核心,也是最为复杂的工程
---
模糊查询(向量) vs 精准查询(关键词)
---
chunks大小取决颗粒度
小 300-500 👉🏻 适合回答细节问题
中 600-800 👉🏻 适中
大 1000 👉🏻 适合回答整体问题/宏观的问题
---
如果召回两个chunk,内容类似,但是含义不相似怎么办?
- Embedding模型的初筛能力较差, 都会召回, 让后面的llm继续深入分析
- Embedding模型筛选的时候,会漏掉一些数据,这个不可避免
---
**文档处理流程**
一.文档的预处理
  **PDF文件 → 文本提取 → 文本分割 → 页码映射**
  1 文本提取
    - 逐页提取文本内容
    - 记录每行文本对应的页码信息
    - 处理空间也异常的情况
  2 文本分割策略
    - 使用递归字符分割器
    - 分割参数: overlap + 长度
    - 分割策略优先级: 段落 → 句子 → 文字 → 空格 → 字符
  3 页面的提取与映射策略
    - 基于字符位置计算每个文本框的页码
    - 建立文本块与页面之间的联系
二.知识库构建
  **文本块 → 嵌入向量 → Faiss索引 → 本地持久化**
  1 向量数据库构建
    - 使用DashScope嵌入模型生成向量
    - 将向量存储到Faiss索引结构
  2 数据持久化
    - 保存Faiss索引文件 (.faiss)
    - 保存元数据信息 (.pkl)
    - 保存页码映射关系 (page_info.pkl)
三.问答查询
  **用户问题 → 向量检索 → 文档组合 → LLM生成 → 答案输出**
  1 相似度检索
    - 将用户问题转换为向量
    - 在Faiss中搜索最相似的文档块，返回Top-K相关文档
  2 问答链处理
    - 使用LangChain的load_qa_chain; qa-chain:问答模型,帮助回答问题
    - 采用 stuff 策略组合文档
    - 将组合后的上下文和问题发送给LLM
  3 答案生成与展示





