1. æœ¬åœ° Ollama æ¥å…¥è¯´æ˜
2. Python / Node.js ä½¿ç”¨ç¤ºä¾‹
3. cURL æµ‹è¯•
4. ä¸€ä¸ª **å°è£…å¥½çš„ Python Client ç±»**ï¼ˆæ”¯æŒåˆ‡æ¢æ¨¡å‹ï¼‰

------

# ğŸ“˜ Ollama æœ¬åœ°æ¥å…¥æŒ‡å—

## 1. ç¡®è®¤æœ¬åœ°æ¨¡å‹

æŸ¥çœ‹ä½ æœ¬åœ°å·²ç»å®‰è£…çš„æ¨¡å‹ï¼š

```bash
ollama list
```

ç¤ºä¾‹è¾“å‡ºï¼š

```
NAME               ID              SIZE      MODIFIED
qwen3:14b          bdbd181c33f2    9.3 GB    4 months ago
llama3.2:latest    a80c4f17acd5    2.0 GB    4 months ago
```

------

## 2. å¯åŠ¨æœåŠ¡

Ollama é»˜è®¤ä¼šåœ¨åå°ç›‘å¬ `http://localhost:11434`ï¼Œå¦‚éœ€æ‰‹åŠ¨å¯åŠ¨ï¼š

```bash
ollama serve
# æŸ¥çœ‹ç«¯å£å ç”¨æƒ…å†µï¼ˆçœ‹ä¸‹è¢«å“ªä¸ªæœåŠ¡å ç”¨äº†ï¼Œé»˜è®¤çš„ollamaæ˜¯11434ï¼‰
lsof -i :11434
```

API å…¥å£ä¸ºï¼š

```shell
http://localhost:11434/v1

# åˆ—å‡ºæœ¬åœ°æ¨¡å‹
curl http://127.0.0.1:11434/v1/models

# è¯·æ±‚ç¤ºä¾‹
curl -N http://127.0.0.1:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:latest",
    "messages": [
      {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹Node.js"}
    ]
  }'


```

------

## 3. Python æ¥å…¥

### å®‰è£…ä¾èµ–

```bash
pip install openai
```

### ç®€å•è°ƒç”¨ç¤ºä¾‹

```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")

response = client.chat.completions.create(
    model="qwen3:14b",
    messages=[{"role": "user", "content": "å†™ä¸€ä¸ª Python æ‰“å°ä¹ä¹ä¹˜æ³•è¡¨çš„ç¨‹åº"}],
)

print(response.choices[0].message["content"])
```

âš ï¸ æ³¨æ„ï¼š

- `api_key` æœ¬åœ°ä¸éªŒè¯ï¼Œå¯ä»¥éšä¾¿å†™ï¼Œä¾‹å¦‚ `"ollama"`ã€‚
- `model` è¦ä¸ `ollama list` ä¸­çš„æ¨¡å‹åç§°ä¸€è‡´ã€‚

------

## 4. Node.js / JavaScript æ¥å…¥

### å®‰è£…ä¾èµ–

```bash
npm install openai
```

### ç¤ºä¾‹

```javascript
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:11434/v1",
  apiKey: "ollama", // æœ¬åœ°ä¸éªŒè¯ï¼Œå¯ä»¥éšä¾¿å†™
});

const response = await client.chat.completions.create({
  model: "llama3.2:latest",
  messages: [{ role: "user", content: "å†™ä¸€ä¸ª JS ç‰ˆæ–æ³¢é‚£å¥‘å‡½æ•°" }],
});

console.log(response.choices[0].message.content);
```

------

## 5. cURL æµ‹è¯•

å¿«é€Ÿç¡®è®¤æœåŠ¡æ­£å¸¸ï¼š

```bash
curl http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3:14b",
    "messages": [{"role": "user", "content": "Hello, Ollama!"}]
  }'
```

------

## 6. å°è£…å¥½çš„ Python Client ç±»

```python
from openai import OpenAI

class OllamaClient:
    def __init__(self, base_url="http://localhost:11434/v1", api_key="ollama"):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model = None

    def set_model(self, model_name: str):
        """åˆ‡æ¢å½“å‰æ¨¡å‹"""
        self.model = model_name

    def chat(self, prompt: str, system: str = None) -> str:
        """å‘é€å¯¹è¯è¯·æ±‚"""
        if not self.model:
            raise ValueError("è¯·å…ˆè°ƒç”¨ set_model() è®¾ç½®æ¨¡å‹")
        
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})

        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages
        )
        return response.choices[0].message["content"]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    ollama = OllamaClient()
    
    # åˆ‡æ¢åˆ° qwen3:14b
    ollama.set_model("qwen3:14b")
    print(ollama.chat("ç”¨ Python æ‰“å°ä¹ä¹ä¹˜æ³•è¡¨"))
    
    # åˆ‡æ¢åˆ° llama3.2:latest
    ollama.set_model("llama3.2:latest")
    print(ollama.chat("ç”¨ JavaScript å†™ä¸€ä¸ªå¿«é€Ÿæ’åºå‡½æ•°"))
```

------

## 7. æ€»ç»“

- Ollama æä¾› OpenAI å…¼å®¹ APIï¼Œæœ¬åœ°é»˜è®¤ç«¯å£ï¼š`http://localhost:11434/v1`
- `api_key` æœ¬åœ°ç¯å¢ƒä¸æ ¡éªŒï¼Œå¯éšä¾¿å¡«
- Python å’Œ Node.js éƒ½èƒ½ç›´æ¥ä½¿ç”¨ `openai` SDK è°ƒç”¨
- å°è£…ç±»å¯æ–¹ä¾¿åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼ˆå¦‚ `qwen3:14b`ã€`llama3.2:latest`ï¼‰

------

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠè¿™ä¸ª `.md` æ–‡ä»¶å¯¼å‡ºæˆä¸€ä¸ª **å¯ä¸‹è½½çš„ Markdown æ–‡ä»¶**ï¼Œè¿™æ ·ä½ ç›´æ¥å­˜åˆ°æœ¬åœ°å°±èƒ½éšæ—¶æ‰“å¼€ï¼Ÿ